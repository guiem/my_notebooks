{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Interesting concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* ### Learning\n",
    "    * [Learning rate decay](#rate_decay)\n",
    "    * [Dropout](#dropout)\n",
    "    * [Batch normalization](#batch)\n",
    "    * [ReLU](#relu)\n",
    "* ### NLP\n",
    "    * [Distributional Hypothesis](#distrib_hyp)\n",
    "    * [CBOW](#cbow)\n",
    "    * [Skip Gram](#skip_gram)\n",
    "    * [Perplexity](#perplexity)\n",
    "    * [LSTM](#lstm)\n",
    "    * [GRU](#gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay <a class=\"anchor\" id=\"rate_decay\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model, it is often recommended to lower the learning rate as the training progresses. If you are going to fast you might end up jumping from one side of the valley to the other. \n",
    "Slowing your learning rate by a certain factor will also slow down the process of learning, which is not desirable. Solution: __start fast and slowly decay__ your learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout <a class=\"anchor\" id=\"dropout\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization <a class=\"anchor\" id=\"batch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization potentially helps in two ways: faster learning and higher overall accuracy. The improved method also allows you to use a higher learning rate, potentially providing another boost in speed.\n",
    "\n",
    "Normalization (shifting inputs to zero-mean and unit variance) is often used as a pre-processing step to make the data comparable across features. As the data flows through a deep network, the weights and parameters adjust those values, sometimes making the data too big or too small again - a problem the authors refer to as \"internal covariate shift\". By normalizing the data in each mini-batch, this problem is largely avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU <a class=\"anchor\" id=\"relu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function defined as the positive part of its argument. This activation is based on strong biological motivations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Hypothesis <a class=\"anchor\" id=\"distrib_hyp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States that words that appear in the same contexts share semantic meaning (linguistic items with similar distributions have similar meanings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW <a class=\"anchor\" id=\"cbow\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram <a class=\"anchor\" id=\"skip_gram\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over. They provide one way of overcoming the data sparsity problem found with conventional n-gram analysis.\n",
    "\n",
    "Formally, an $n$-gram is a consecutive subsequence of length $n$ of some sequence of tokens $w_1 â€¦ w_n$. A $k-skip-n-gram$ is a length-$n$ subsequence where the components occur at distance at most $k$ from each other.\n",
    "\n",
    "For example, in the input text:\n",
    "\n",
    "*the rain in Spain falls mainly on the plain*\n",
    "\n",
    "the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences\n",
    "\n",
    "*the in, rain Spain, in falls, Spain mainly, falls on, mainly the, and on plain.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Perplexity <a class=\"anchor\" id=\"perplexity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. A low perplexity indicates the probability distribution is good at predicting the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LSTM <a class=\"anchor\" id=\"lstm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GRU <a class=\"anchor\" id=\"gru\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.\n",
    "\n",
    "They have fewer parameters than LSTM, as they lack an output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
